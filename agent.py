from langchain_core.tools import tool
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from dotenv import load_dotenv
import tools.fda_api as fda_api
import tools.financial_report_rag as financial_report_rag
import tools.neo4j_rag as neo4j_rag


# Load environment variables from .env file
load_dotenv()

# This tool is used to query the FDA API for adverse events related to a drug
@tool
def fda_tool(drug_name: str) -> str:
    """Query the FDA Adverse Events API for adverse consequences or adverse interactions and events for a drug (e.g. 'ibuprofen')."""
    return fda_api.query_fda(drug_name)

#This tool is used to query the financial report PDF for Grünenthal for the years 2023 and 2024
@tool
def financial_rag_tool(question: str) -> str:
    """Answer questions from Grünenthal’s financial report PDF fpr the years 2023 and 2024."""
    return financial_report_rag.query_financial_report(question)

#This tool is used to query the Neo4j Healthcare Analytics graph database for medical questions
@tool
def neo4j_tool(query: str) -> str:
    """Answer questions about drug adverse event cases, drug manufacturers and outcomes from the Neo4j Healthcare Analytics medical graph database by transforming questions into Cypher queries and building human responses from the results."""
    return neo4j_rag.query_neo4j(query)

# Define the tools to be used
tools = [financial_rag_tool, neo4j_tool, fda_tool]

AI_AGENT_TEMPLATE = '''
You are a helpful AI assistant that can answer questions about Grünenthal's financial report, the Neo4j Healthcare Analytics graph database, and FDA adverse events related to drugs. 
You should use the following tools to answer questions:  {tools}
You can use your own knowledge if you cannot get enough information from the tools.

Example query: Compare Grünenthal’s performance to industry benchmarks, if available.

In this case, you can use the financial report tool to answer questions about Grünenthal's financial performance, and your own knowledge to compare it to industry benchmarks.
'''

# Initialize the Google Generative AI model
llm = init_chat_model(
    "gemini-2.0-flash", 
    model_provider="google_genai",
    temperature=0.5,
    max_tokens=512)

llm_with_tools = llm.bind_tools(tools)

'''This function executes the agent with the provided query, handling tool calls and returning the final response. 
It uses the Google Gemini Generative AI model to process the query and any tool calls made during the conversation.

Args:
    query (str): The user query to be processed by the agent.
Returns:
    str: The final response generated by the agent after processing the query and any tool calls.
'''
async def execute_agent(query: str) -> str:
    messages = [SystemMessage(AI_AGENT_TEMPLATE), HumanMessage(query)]

    ai_msg = llm_with_tools.invoke(messages)

    messages.append(ai_msg)

    for tool_call in ai_msg.tool_calls:
        selected_tool = {"neo4j_tool": neo4j_tool, "fda_tool": fda_tool, "financial_rag_tool": financial_rag_tool}[tool_call["name"].lower()]
        tool_msg = selected_tool.invoke(tool_call)
        messages.append(tool_msg)
    
    print(messages)
    
    return llm_with_tools.invoke(messages).content
